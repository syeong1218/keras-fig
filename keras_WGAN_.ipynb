{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_WGAN .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYdG5IJXEanHq7+qR9rQ/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syeong1218/keras-fig/blob/master/keras_WGAN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGeCxQfuNv4W",
        "colab_type": "text"
      },
      "source": [
        "# 5장 개선된 GAN 모델\n",
        "\n",
        "- WGAN의 이론적 방정식\n",
        "- LSGAN 이론\n",
        "- ACGAN 이론\n",
        "- 케라스를 사용해 개선된 GAN 모델을 구현하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyaLerHLN0WG",
        "colab_type": "text"
      },
      "source": [
        "### GAN\n",
        "판별기는 진짜 데이터에서 가짜 데이터를 정확하게 분류해내려고 하고, 생성기는 판별기를 속이려한다. 판별기와 생성기가 서로 상반된 목표를 가지고 있어 훈련할 때 쉽게 불안정해진다.\n",
        "\n",
        "GAN의 목표는 생성기 데이터 분포를 진짜 데이터 분포로 만드는 것이다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPEAUtDTXFG",
        "colab_type": "text"
      },
      "source": [
        "###  WGAN\n",
        "\n",
        "판별기와 생성기를 교대로 훈련시킨다. 생성기를 1회 훈련시키기 전에 판별기를 $n_{critic}$회 훈련시킨다. 이는 생성기와 판별기를 동일한 횟수로 훈련시키는 GAN과 다른 점이다.\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/keras-fig/blob/master/5.1.3.JPG?raw=true)\n",
        "> **알고리즘**<br>\n",
        "  매개변수 : $\\alpha=0.00005, c=0.01, m=64, n_{critic}=5$ <br>\n",
        "  조건 : $\\alpha$- 학습속도, c-클리핑 매개변수, m-배치크기, $n_{critic}$-생성기가 1회 반복하는 동안 판별기의 반복 횟수<br>\n",
        "  조건 : $w_0$-초기 판별기 매개변수, $\\theta_0$-초기 생성기 매개변수<br>\n",
        "  1. while $\\theta$가 수렴하지 않는 동안 do\n",
        "  2. for t=1,...,$n_{critic}$ do\n",
        "  3. 실제 데이터에서 배치 ${(x^{(i)})}_{i=1}^m \\sim p_{data}$를 샘플링\n",
        "  4. 균등 노이즈 분포에서 배치 ${(z^{(i)})}_{i=1}^m \\sim p(z)$를 샘플링\n",
        "  5. 판별기 경사를 계산,$g_w\\leftarrow \\triangledown _w\\left [ -\\frac{1}{m}\\sum_{i=1}^{m}D_w(x^{(i)})+\\frac{1}{m}\\sum_{i=1}^{m}D_w(g_\\theta(z^{(i)})) \\right ]$\n",
        "  6. 판별기 매개변수 업데이트,$w\\leftarrow w-\\alpha \\times RMSProp(w,g_w)$\n",
        "  7. 판별기 가중치 제한, $w\\leftarrow clip(w,-c,c)$\n",
        "  8. end for\n",
        "  9. 균등 노이즈 분포에서 배치 ${(z^{(i)})}_{i=1}^m \\sim p(z)$를 샘플링\n",
        "  10. 생성기 경사 계산, $g_{\\theta}\\leftarrow \\triangledown _{\\theta}\\frac{1}{m}\\sum_{i=1}^{m}D_w(g_\\theta(z^{(i)}))$\n",
        "  11. 생성기 매개변수 업데이트, $\\theta\\leftarrow \\theta-\\alpha \\times RMSProp(w,g_{\\theta})$\n",
        "  12. end while\n",
        ".\n",
        "\n",
        "판별기를 훈련시킨다는 것은 판별기의 매개변수를 학습한다는 뜻이다.\n",
        ">>**판별기 훈련**\n",
        "<br>3. 실제 데이터에서 배치를 샘플링\n",
        " <br>4. 가짜 데이터에서 배치를 샘플링\n",
        " <br>5. 샘플링된 데이터를 판별기 네트워크에 공급한 다음, 판별기 매개변수의 경사를 계산\n",
        " <br>6. 판별기 매개변수는 RMSProp를 사용해 최적화\n",
        " <br>7. EMD 최적화에서 판별기 매개변수를 특정 범위 내로 제한해서 립시츠의 제약 조건을 적용\n",
        " <br><br>실제 데이터를 사용해 훈련할 때 손실 함수를 최소화하기 위해 $y_{prediction}=D_{w}(x)$를 증가시킨다. 가짜 데이터를 사용해 훈련시키는 경우, $y_{prediction}=D_{w}(G(x))$를 감소시켜 손실 함수를 최소화한다.\n",
        "\n",
        ">>**생성기 훈련**\n",
        " <br>9. 가짜 데이터 배치를 샘플링\n",
        " <br>10. 생성기 경사 계산\n",
        " <br>11. RMSProp를 사용해 최적화\n",
        " <br><br> $y_{prediction}=D_{w}(G(x))$를 증가시켜 훈련하는 동안 가짜 데이터에 진짜 레이블이 달려 있을 때 손실함수를 최소화한다. \n",
        " \n",
        "생성기 훈련이 끝나면 판별기 매개변수를 고정했던 것을 풀고 다시 판별기 훈련을 $n_{critic}$회 시작한다. \n",
        "\n",
        "생성기는 데이터 위조에만 관여하기 때문에 판별기를 훈련하는 동안 생성기 매개변수를 고정할 필요가 없다.\n",
        "\n",
        "판별기 경사를 계산할 때 실제 데이터의 레이블은 1.0이지만 가짜 데이터의 레이블은 -1.0이 된다.\n",
        "\n",
        "\n",
        "- RMSProp 사용 이유\n",
        "\n",
        "  critic을 학습 할 때 Adam과 같은 mometum 베이스 optimizer를 사용하면 학습이 불안정하다 \n",
        "\n",
        "  그 이유는 loss값이 튀고 샘플이 좋지 않은 경우(일반적으로 학습 초반) Adam이 가고자 하는 방향, 즉 이전에 기억했던 방향(Adam step)과 gradient의 방향 간의 cosine값이 음수가 된다. 일반적으로 nonstationary 문제(극한값이 존재하지 않음)에 대해서는 momentum계열보다 RMSProp이 성능이 더 좋다고 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCuELQxFkzX4",
        "colab_type": "code",
        "outputId": "d88ab602-940e-4ab0-d64b-3e620811321f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "try:\n",
        "\n",
        "    %tensorflow_version 1.x  # %tensorflow_version only exists in Colab\n",
        "\n",
        "except Exception:\n",
        "\n",
        "    pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x  # %tensorflow_version only exists in Colab`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfZRUlxlu60t",
        "colab_type": "code",
        "outputId": "5f0b9f6a-4ec3-45bd-a4aa-ae624a9fd00f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "from google.colab import files\n",
        "src=list(files.upload().values())[0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7adb95a6-2237-4750-b9c8-75c8c83588d3\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7adb95a6-2237-4750-b9c8-75c8c83588d3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving gan.py to gan.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0NabfsYNuEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8cde8922-5608-4396-8303-fe9a7f0dcdb2"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import sys\n",
        "import gan\n",
        "\n",
        "def train(models, x_train, params):\n",
        "    \"\"\"판별기와 적대적 네트워크 훈련\n",
        "    판별기와 적대적 네트워크를 배치 단위로 교대로 훈련\n",
        "    먼저 판별기가 제대로 레이블이 붙은 진짜와 가짜 이미지를 사용해 n_critic회 훈련됨\n",
        "    판별기 가중치는 립시츠 제약 조건에 따라 범위가 제한됨\n",
        "    다음으로 생성기가 진짜인 척하는 가짜 이미지를 사용해(적대적 네트워크를 통해) 훈련됨\n",
        "    save_interval마다 샘플 이미지를 생성\n",
        "\n",
        "    #인수\n",
        "     models(list) : Generator, discriminator, Adversarial 모델\n",
        "     x_train(tensor) : 이미지 훈련\n",
        "     params(list) : 네트워크 매개변수\n",
        "    \"\"\"\n",
        "    # GAN 모델\n",
        "    generator, discriminator, adversarial = models\n",
        "    # 네트워크 매개변수\n",
        "    (batch_size, latent_size, n_critic, \n",
        "            clip_value, train_steps, model_name) = params\n",
        "    # 생성기 이미지는 500 단계마다 저장됨\n",
        "    save_interval = 500\n",
        "    # 훈련하는 동안 생성기 출력이 어떻게 진화하는지 보기 위한 노이즈 벡터\n",
        "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
        "    # 훈련 데이터 세트의 요소 개수\n",
        "    train_size = x_train.shape[0]\n",
        "    # 실제 데이터의 레이블\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    for i in range(train_steps):\n",
        "        # 판별기를 n_critic회 훈련시킴\n",
        "        loss = 0\n",
        "        acc = 0\n",
        "        for _ in range(n_critic):\n",
        "            # 1 배치에 대해 판별기를 훈련\n",
        "            # 실제 이미지(label=1.0)와 가짜 이미지(label=-1.0)으로 구성된 1 배치\n",
        "            # 데이터세트에서 실제 이미지를 임의로 선정\n",
        "            rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
        "            real_images = x_train[rand_indexes]\n",
        "            # 생성기를 사용해 노이즈에서 가짜 이미지 생성\n",
        "            # 균등 분포를 사용해 노이즈 생성\n",
        "            noise = np.random.uniform(-1.0,\n",
        "                                      1.0,\n",
        "                                      size=[batch_size, latent_size])\n",
        "            fake_images = generator.predict(noise)\n",
        "\n",
        "            # 판별기 네트워크 훈련\n",
        "            # 진짜 데이터 레이블l=1, 가짜 데이터 레이블=-1\n",
        "            # 진짜와 가짜 이미지를 결합해 하나의 배치를 만드는 대신\n",
        "            # 처음에는 진짜 데이터로 구성된 하나의 배치로 훈련한 다음\n",
        "            # 가짜 이미지로 구성된 하나의 배치로 훈련\n",
        "            # 이렇게 바꿈으로써\n",
        "            # 진짜와 가짜 데이터 레이블의 부호가 반대고(+1과 -1)\n",
        "            # 점위 제한(클리핑)으로 인해 가중치의 크기가 작아서\n",
        "            # 경사가 소실되는 것을 방지\n",
        "            real_loss, real_acc = discriminator.train_on_batch(real_images,\n",
        "                                                               real_labels)\n",
        "            fake_loss, fake_acc = discriminator.train_on_batch(fake_images,\n",
        "                                                               -real_labels)\n",
        "            # 평균 손실과 정확도를 누적\n",
        "            loss += 0.5 * (real_loss + fake_loss)\n",
        "            acc += 0.5 * (real_acc + fake_acc)\n",
        "\n",
        "            # 립시츠 제약 사향을 만족하기 위해 판별치 가중치 범위 제한\n",
        "            for layer in discriminator.layers:\n",
        "                weights = layer.get_weights()\n",
        "                weights = [np.clip(weight,\n",
        "                                   -clip_value,\n",
        "                                   clip_value) for weight in weights]\n",
        "                layer.set_weights(weights)\n",
        "\n",
        "        # n_critic회 반복 훈련하는 동안 평균 손실과 정확도\n",
        "        loss /= n_critic\n",
        "        acc /= n_critic\n",
        "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
        "\n",
        "        # 1 배치 동안 적대적 네트워크 훈련\n",
        "        # label=1.0인 가짜 이미지의 1 배치\n",
        "        # 적대적 네트워크의 판별기 가중치가 고정되어 있으므로\n",
        "        # 생성기만 훈련됨\n",
        "        # 균등 분포를 사용해 노이즈 생성\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "        # 적대적 네트워크 훈련\n",
        "        # 판별기 훈련과 달리 변수에 가짜 이미지를 저장하지 않음\n",
        "        # 가짜 이미지는 분류를 위해 적대적 네트워크의 판별기 입력으로 전달됨\n",
        "        # 가짜 이미지는 진짜 레이블을 가지고 있음\n",
        "        # 손실과 정확도를 기록\n",
        "        loss, acc = adversarial.train_on_batch(noise, real_labels)\n",
        "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
        "        print(log)\n",
        "        if (i + 1) % save_interval == 0:\n",
        "            if (i + 1) == train_steps:\n",
        "                show = True\n",
        "            else:\n",
        "                show = False\n",
        "\n",
        "            # 주기적으로 생성기 이미지를 그림\n",
        "            gan.plot_images(generator,\n",
        "                            noise_input=noise_input,\n",
        "                            show=show,\n",
        "                            step=(i + 1),\n",
        "                            model_name=model_name)\n",
        "\n",
        "    # 생성기 훈련이 끝나면 모델을 저장\n",
        "    # 훈련기 생성기는 향후 MNIST 숫자 생성을 위해 재로딩될 수 있음\n",
        "    generator.save(model_name + \".h5\")\n",
        "\n",
        "\n",
        "def wasserstein_loss(y_label, y_pred):\n",
        "    return -K.mean(y_label * y_pred)\n",
        "\n",
        "\n",
        "def build_and_train_models():\n",
        "    # MNIST 데이터세트 로딩\n",
        "    (x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "    # CNN 데이터를 (28, 28, 1)로 형상을 변경하고 정규화함\n",
        "    image_size = x_train.shape[1]\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "    x_train = x_train.astype('float32') / 255\n",
        "\n",
        "    model_name = \"wgan_mnist\"\n",
        "    # 네트워크 매개변수\n",
        "    # 잠재 혹은 z벡터 차원은 100\n",
        "    latent_size = 100\n",
        "    # WGAN 논문[2]에서 참조한 매개변수\n",
        "    n_critic = 5\n",
        "    clip_value = 0.01\n",
        "    batch_size = 64\n",
        "    lr = 5e-5\n",
        "    train_steps = 400\n",
        "    input_shape = (image_size, image_size, 1)\n",
        "\n",
        "    # 판별기 모델 구성\n",
        "    inputs = Input(shape=input_shape, name='discriminator_input')\n",
        "    # WGAN은 논문[2]의 선형 활성화를 사용\n",
        "    discriminator = gan.discriminator(inputs, activation='linear')\n",
        "    optimizer = RMSprop(lr=lr)\n",
        "    # WGAN 판별기는 베셔슈타인 손실을 사용\n",
        "    discriminator.compile(loss=wasserstein_loss,\n",
        "                          optimizer=optimizer,\n",
        "                          metrics=['accuracy'])\n",
        "    discriminator.summary()\n",
        "\n",
        "    # 생성기 모델 구성\n",
        "    input_shape = (latent_size, )\n",
        "    inputs = Input(shape=input_shape, name='z_input')\n",
        "    generator = gan.generator(inputs, image_size)\n",
        "    generator.summary()\n",
        "\n",
        "    # 적대적 모델 생성 = 생성기 + 판별기\n",
        "    # 적대적 네트워크를 훈련하는 동안 판별기의 가중치는 고정\n",
        "    discriminator.trainable = False\n",
        "    adversarial = Model(inputs,\n",
        "                        discriminator(generator(inputs)),\n",
        "                        name=model_name)\n",
        "    adversarial.compile(loss=wasserstein_loss,\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    adversarial.summary()\n",
        "\n",
        "    # 판별기와 적대적 네트워크를 훈련\n",
        "    models = (generator, discriminator, adversarial)\n",
        "    params = (batch_size,\n",
        "              latent_size,\n",
        "              n_critic,\n",
        "              clip_value,\n",
        "              train_steps,\n",
        "              model_name)\n",
        "    train(models, x_train, params)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    help_ = \"Load generator h5 model with trained weights\"\n",
        "    parser.add_argument(\"-g\", \"--generator\", help=help_)\n",
        "    args = parser.parse_args(args=[])\n",
        "    if args.generator:\n",
        "        generator = load_model(args.generator)\n",
        "        gan.test_generator(generator)\n",
        "    else:\n",
        "        build_and_train_models()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear\n",
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "discriminator_input (InputLa (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 32)        832       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 64)          51264     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 4097      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 1,080,577\n",
            "Trainable params: 1,080,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "z_input (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTr (None, 14, 14, 128)       409728    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 28, 28, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 1)         801       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,301,505\n",
            "Trainable params: 1,300,801\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n",
            "Model: \"wgan_mnist\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "z_input (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "generator (Model)            (None, 28, 28, 1)         1301505   \n",
            "_________________________________________________________________\n",
            "discriminator (Model)        (None, 1)                 1080577   \n",
            "=================================================================\n",
            "Total params: 2,382,082\n",
            "Trainable params: 1,300,801\n",
            "Non-trainable params: 1,081,281\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: [discriminator loss: 0.011445, acc: 0.000000] [adversarial loss: -0.000029, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1: [discriminator loss: -0.000068, acc: 0.000000] [adversarial loss: 0.000197, acc: 0.000000]\n",
            "2: [discriminator loss: -0.000214, acc: 0.000000] [adversarial loss: 0.000566, acc: 0.000000]\n",
            "3: [discriminator loss: -0.000507, acc: 0.000000] [adversarial loss: 0.001397, acc: 0.000000]\n",
            "4: [discriminator loss: -0.001106, acc: 0.000000] [adversarial loss: 0.003198, acc: 0.000000]\n",
            "5: [discriminator loss: -0.002618, acc: 0.000000] [adversarial loss: 0.007528, acc: 0.000000]\n",
            "6: [discriminator loss: -0.005701, acc: 0.000000] [adversarial loss: 0.015940, acc: 0.000000]\n",
            "7: [discriminator loss: -0.011364, acc: 0.000000] [adversarial loss: 0.029976, acc: 0.000000]\n",
            "8: [discriminator loss: -0.021067, acc: 0.000000] [adversarial loss: 0.051825, acc: 0.000000]\n",
            "9: [discriminator loss: -0.034554, acc: 0.000000] [adversarial loss: 0.080696, acc: 0.000000]\n",
            "10: [discriminator loss: -0.050379, acc: 0.000000] [adversarial loss: 0.113871, acc: 0.000000]\n",
            "11: [discriminator loss: -0.073137, acc: 0.000000] [adversarial loss: 0.154363, acc: 0.000000]\n",
            "12: [discriminator loss: -0.102311, acc: 0.000000] [adversarial loss: 0.201821, acc: 0.000000]\n",
            "13: [discriminator loss: -0.140954, acc: 0.000000] [adversarial loss: 0.263731, acc: 0.000000]\n",
            "14: [discriminator loss: -0.186890, acc: 0.000000] [adversarial loss: 0.338317, acc: 0.000000]\n",
            "15: [discriminator loss: -0.245449, acc: 0.000000] [adversarial loss: 0.423223, acc: 0.000000]\n",
            "16: [discriminator loss: -0.313386, acc: 0.010937] [adversarial loss: 0.523975, acc: 0.000000]\n",
            "17: [discriminator loss: -0.405190, acc: 0.235937] [adversarial loss: 0.650401, acc: 0.000000]\n",
            "18: [discriminator loss: -0.492161, acc: 0.681250] [adversarial loss: 0.795952, acc: 0.000000]\n",
            "19: [discriminator loss: -0.609477, acc: 0.795312] [adversarial loss: 0.952492, acc: 0.000000]\n",
            "20: [discriminator loss: -0.689386, acc: 0.803125] [adversarial loss: 1.112949, acc: 0.000000]\n",
            "21: [discriminator loss: -0.809586, acc: 0.828125] [adversarial loss: 1.316075, acc: 0.000000]\n",
            "22: [discriminator loss: -0.937003, acc: 0.840625] [adversarial loss: 1.524274, acc: 0.000000]\n",
            "23: [discriminator loss: -1.107162, acc: 0.718750] [adversarial loss: 1.765341, acc: 0.000000]\n",
            "24: [discriminator loss: -1.236560, acc: 0.410938] [adversarial loss: 2.000331, acc: 0.000000]\n",
            "25: [discriminator loss: -1.402248, acc: 0.264062] [adversarial loss: 2.271748, acc: 0.000000]\n",
            "26: [discriminator loss: -1.548588, acc: 0.226562] [adversarial loss: 2.607295, acc: 0.000000]\n",
            "27: [discriminator loss: -1.705592, acc: 0.170313] [adversarial loss: 2.885949, acc: 0.000000]\n",
            "28: [discriminator loss: -1.875699, acc: 0.153125] [adversarial loss: 3.279657, acc: 0.000000]\n",
            "29: [discriminator loss: -2.044962, acc: 0.112500] [adversarial loss: 3.576907, acc: 0.000000]\n",
            "30: [discriminator loss: -2.331233, acc: 0.139063] [adversarial loss: 4.003545, acc: 0.000000]\n",
            "31: [discriminator loss: -2.572029, acc: 0.095312] [adversarial loss: 4.449395, acc: 0.000000]\n",
            "32: [discriminator loss: -2.791808, acc: 0.087500] [adversarial loss: 4.913332, acc: 0.000000]\n",
            "33: [discriminator loss: -3.085961, acc: 0.056250] [adversarial loss: 5.399701, acc: 0.000000]\n",
            "34: [discriminator loss: -3.369865, acc: 0.064062] [adversarial loss: 5.968176, acc: 0.000000]\n",
            "35: [discriminator loss: -3.753638, acc: 0.050000] [adversarial loss: 6.612479, acc: 0.000000]\n",
            "36: [discriminator loss: -4.110684, acc: 0.051562] [adversarial loss: 7.301139, acc: 0.000000]\n",
            "37: [discriminator loss: -4.402842, acc: 0.075000] [adversarial loss: 7.845346, acc: 0.000000]\n",
            "38: [discriminator loss: -4.826387, acc: 0.032813] [adversarial loss: 8.649270, acc: 0.000000]\n",
            "39: [discriminator loss: -5.026366, acc: 0.054688] [adversarial loss: 9.382668, acc: 0.000000]\n",
            "40: [discriminator loss: -4.614309, acc: 0.040625] [adversarial loss: 9.140384, acc: 0.000000]\n",
            "41: [discriminator loss: -2.478177, acc: 0.067187] [adversarial loss: 5.911094, acc: 0.000000]\n",
            "42: [discriminator loss: -2.076618, acc: 0.204687] [adversarial loss: 4.855410, acc: 0.000000]\n",
            "43: [discriminator loss: -2.312349, acc: 0.065625] [adversarial loss: 5.367624, acc: 0.000000]\n",
            "44: [discriminator loss: -2.655188, acc: 0.039062] [adversarial loss: 5.969869, acc: 0.000000]\n",
            "45: [discriminator loss: -2.846354, acc: 0.046875] [adversarial loss: 6.543815, acc: 0.000000]\n",
            "46: [discriminator loss: -2.859628, acc: 0.042188] [adversarial loss: 6.799714, acc: 0.000000]\n",
            "47: [discriminator loss: -3.110122, acc: 0.068750] [adversarial loss: 7.039723, acc: 0.000000]\n",
            "48: [discriminator loss: -3.579324, acc: 0.053125] [adversarial loss: 7.491572, acc: 0.000000]\n",
            "49: [discriminator loss: -3.775559, acc: 0.040625] [adversarial loss: 7.705559, acc: 0.000000]\n",
            "50: [discriminator loss: -3.875701, acc: 0.045312] [adversarial loss: 7.886061, acc: 0.000000]\n",
            "51: [discriminator loss: -4.377478, acc: 0.029687] [adversarial loss: 8.235536, acc: 0.000000]\n",
            "52: [discriminator loss: -4.485062, acc: 0.023438] [adversarial loss: 8.540440, acc: 0.000000]\n",
            "53: [discriminator loss: -4.902301, acc: 0.029687] [adversarial loss: 8.903114, acc: 0.000000]\n",
            "54: [discriminator loss: -5.148638, acc: 0.029687] [adversarial loss: 9.389086, acc: 0.000000]\n",
            "55: [discriminator loss: -5.462864, acc: 0.037500] [adversarial loss: 10.001518, acc: 0.000000]\n",
            "56: [discriminator loss: -5.945940, acc: 0.037500] [adversarial loss: 10.718745, acc: 0.000000]\n",
            "57: [discriminator loss: -6.125189, acc: 0.025000] [adversarial loss: 11.228706, acc: 0.000000]\n",
            "58: [discriminator loss: -6.582263, acc: 0.020313] [adversarial loss: 11.946325, acc: 0.000000]\n",
            "59: [discriminator loss: -6.996608, acc: 0.020313] [adversarial loss: 12.541601, acc: 0.000000]\n",
            "60: [discriminator loss: -7.537799, acc: 0.020313] [adversarial loss: 13.329844, acc: 0.000000]\n",
            "61: [discriminator loss: -7.961949, acc: 0.023438] [adversarial loss: 14.028916, acc: 0.000000]\n",
            "62: [discriminator loss: -8.237027, acc: 0.021875] [adversarial loss: 14.462379, acc: 0.000000]\n",
            "63: [discriminator loss: -8.844971, acc: 0.014063] [adversarial loss: 15.258543, acc: 0.000000]\n",
            "64: [discriminator loss: -9.283546, acc: 0.023438] [adversarial loss: 15.964645, acc: 0.000000]\n",
            "65: [discriminator loss: -9.678511, acc: 0.018750] [adversarial loss: 16.789770, acc: 0.000000]\n",
            "66: [discriminator loss: -10.221405, acc: 0.012500] [adversarial loss: 17.651245, acc: 0.000000]\n",
            "67: [discriminator loss: -10.824552, acc: 0.018750] [adversarial loss: 18.519665, acc: 0.000000]\n",
            "68: [discriminator loss: -11.432508, acc: 0.015625] [adversarial loss: 19.450956, acc: 0.000000]\n",
            "69: [discriminator loss: -11.870846, acc: 0.021875] [adversarial loss: 20.327148, acc: 0.000000]\n",
            "70: [discriminator loss: -12.823837, acc: 0.007812] [adversarial loss: 21.592304, acc: 0.000000]\n",
            "71: [discriminator loss: -13.498359, acc: 0.023438] [adversarial loss: 22.720394, acc: 0.000000]\n",
            "72: [discriminator loss: -13.784257, acc: 0.006250] [adversarial loss: 23.334335, acc: 0.000000]\n",
            "73: [discriminator loss: -13.109846, acc: 0.014063] [adversarial loss: 21.774902, acc: 0.000000]\n",
            "74: [discriminator loss: -6.294629, acc: 0.018750] [adversarial loss: 10.331192, acc: 0.000000]\n",
            "75: [discriminator loss: 0.764193, acc: 0.015625] [adversarial loss: -0.621625, acc: 0.343750]\n",
            "76: [discriminator loss: 2.082682, acc: 0.023438] [adversarial loss: -4.710307, acc: 0.000000]\n",
            "77: [discriminator loss: 2.130657, acc: 0.009375] [adversarial loss: -7.591057, acc: 0.000000]\n",
            "78: [discriminator loss: 1.553823, acc: 0.006250] [adversarial loss: -9.294242, acc: 0.000000]\n",
            "79: [discriminator loss: 1.224929, acc: 0.003125] [adversarial loss: -10.201363, acc: 0.000000]\n",
            "80: [discriminator loss: 0.890194, acc: 0.000000] [adversarial loss: -10.419192, acc: 0.000000]\n",
            "81: [discriminator loss: 0.706633, acc: 0.000000] [adversarial loss: -10.419937, acc: 0.000000]\n",
            "82: [discriminator loss: 0.609217, acc: 0.000000] [adversarial loss: -10.449540, acc: 0.000000]\n",
            "83: [discriminator loss: 0.320996, acc: 0.000000] [adversarial loss: -10.095537, acc: 0.000000]\n",
            "84: [discriminator loss: 0.240615, acc: 0.000000] [adversarial loss: -10.068962, acc: 0.000000]\n",
            "85: [discriminator loss: 0.073727, acc: 0.000000] [adversarial loss: -9.734430, acc: 0.000000]\n",
            "86: [discriminator loss: -0.083323, acc: 0.000000] [adversarial loss: -9.429361, acc: 0.000000]\n",
            "87: [discriminator loss: -0.216784, acc: 0.000000] [adversarial loss: -9.155098, acc: 0.000000]\n",
            "88: [discriminator loss: -0.308027, acc: 0.000000] [adversarial loss: -8.878376, acc: 0.000000]\n",
            "89: [discriminator loss: -0.384597, acc: 0.000000] [adversarial loss: -8.766462, acc: 0.000000]\n",
            "90: [discriminator loss: -0.465884, acc: 0.000000] [adversarial loss: -8.726785, acc: 0.000000]\n",
            "91: [discriminator loss: -0.505365, acc: 0.000000] [adversarial loss: -8.668258, acc: 0.000000]\n",
            "92: [discriminator loss: -0.582342, acc: 0.000000] [adversarial loss: -8.664817, acc: 0.000000]\n",
            "93: [discriminator loss: -0.592380, acc: 0.000000] [adversarial loss: -8.670292, acc: 0.000000]\n",
            "94: [discriminator loss: -0.536465, acc: 0.000000] [adversarial loss: -8.900757, acc: 0.000000]\n",
            "95: [discriminator loss: -0.575531, acc: 0.000000] [adversarial loss: -9.028114, acc: 0.000000]\n",
            "96: [discriminator loss: -0.574621, acc: 0.000000] [adversarial loss: -9.218935, acc: 0.000000]\n",
            "97: [discriminator loss: -0.549933, acc: 0.000000] [adversarial loss: -9.412245, acc: 0.000000]\n",
            "98: [discriminator loss: -0.559849, acc: 0.000000] [adversarial loss: -9.503240, acc: 0.000000]\n",
            "99: [discriminator loss: -0.585692, acc: 0.000000] [adversarial loss: -9.553350, acc: 0.000000]\n",
            "100: [discriminator loss: -0.641033, acc: 0.000000] [adversarial loss: -9.506138, acc: 0.000000]\n",
            "101: [discriminator loss: -0.676517, acc: 0.000000] [adversarial loss: -9.512688, acc: 0.000000]\n",
            "102: [discriminator loss: -0.726607, acc: 0.000000] [adversarial loss: -9.422588, acc: 0.000000]\n",
            "103: [discriminator loss: -0.750556, acc: 0.000000] [adversarial loss: -9.438025, acc: 0.000000]\n",
            "104: [discriminator loss: -0.791751, acc: 0.000000] [adversarial loss: -9.445860, acc: 0.000000]\n",
            "105: [discriminator loss: -0.774650, acc: 0.000000] [adversarial loss: -9.459515, acc: 0.000000]\n",
            "106: [discriminator loss: -0.789031, acc: 0.000000] [adversarial loss: -9.511968, acc: 0.000000]\n",
            "107: [discriminator loss: -0.802059, acc: 0.000000] [adversarial loss: -9.529297, acc: 0.000000]\n",
            "108: [discriminator loss: -0.832098, acc: 0.000000] [adversarial loss: -9.505656, acc: 0.000000]\n",
            "109: [discriminator loss: -0.835683, acc: 0.000000] [adversarial loss: -9.524463, acc: 0.000000]\n",
            "110: [discriminator loss: -0.898429, acc: 0.000000] [adversarial loss: -9.490319, acc: 0.000000]\n",
            "111: [discriminator loss: -0.959360, acc: 0.000000] [adversarial loss: -9.440319, acc: 0.000000]\n",
            "112: [discriminator loss: -0.963356, acc: 0.000000] [adversarial loss: -9.405202, acc: 0.000000]\n",
            "113: [discriminator loss: -0.952043, acc: 0.000000] [adversarial loss: -9.381571, acc: 0.000000]\n",
            "114: [discriminator loss: -0.999511, acc: 0.000000] [adversarial loss: -9.385576, acc: 0.000000]\n",
            "115: [discriminator loss: -1.000523, acc: 0.000000] [adversarial loss: -9.396173, acc: 0.000000]\n",
            "116: [discriminator loss: -0.962441, acc: 0.000000] [adversarial loss: -9.478609, acc: 0.000000]\n",
            "117: [discriminator loss: -1.005162, acc: 0.000000] [adversarial loss: -9.545152, acc: 0.000000]\n",
            "118: [discriminator loss: -1.019670, acc: 0.000000] [adversarial loss: -9.612151, acc: 0.000000]\n",
            "119: [discriminator loss: -1.021671, acc: 0.000000] [adversarial loss: -9.730406, acc: 0.000000]\n",
            "120: [discriminator loss: -1.033523, acc: 0.000000] [adversarial loss: -9.802798, acc: 0.000000]\n",
            "121: [discriminator loss: -1.123594, acc: 0.000000] [adversarial loss: -9.853624, acc: 0.000000]\n",
            "122: [discriminator loss: -1.097537, acc: 0.000000] [adversarial loss: -9.906578, acc: 0.000000]\n",
            "123: [discriminator loss: -1.101706, acc: 0.000000] [adversarial loss: -10.021120, acc: 0.000000]\n",
            "124: [discriminator loss: -1.139130, acc: 0.000000] [adversarial loss: -10.154236, acc: 0.000000]\n",
            "125: [discriminator loss: -1.141480, acc: 0.000000] [adversarial loss: -10.300377, acc: 0.000000]\n",
            "126: [discriminator loss: -1.166016, acc: 0.000000] [adversarial loss: -10.310869, acc: 0.000000]\n",
            "127: [discriminator loss: -1.162437, acc: 0.000000] [adversarial loss: -10.518843, acc: 0.000000]\n",
            "128: [discriminator loss: -1.181529, acc: 0.000000] [adversarial loss: -10.600685, acc: 0.000000]\n",
            "129: [discriminator loss: -1.234106, acc: 0.000000] [adversarial loss: -10.749645, acc: 0.000000]\n",
            "130: [discriminator loss: -1.274691, acc: 0.000000] [adversarial loss: -10.915701, acc: 0.000000]\n",
            "131: [discriminator loss: -1.274720, acc: 0.000000] [adversarial loss: -11.017693, acc: 0.000000]\n",
            "132: [discriminator loss: -1.326984, acc: 0.000000] [adversarial loss: -11.094368, acc: 0.000000]\n",
            "133: [discriminator loss: -1.393859, acc: 0.000000] [adversarial loss: -11.123766, acc: 0.000000]\n",
            "134: [discriminator loss: -1.427735, acc: 0.000000] [adversarial loss: -11.127694, acc: 0.000000]\n",
            "135: [discriminator loss: -1.436241, acc: 0.000000] [adversarial loss: -11.246260, acc: 0.000000]\n",
            "136: [discriminator loss: -1.421650, acc: 0.000000] [adversarial loss: -11.330971, acc: 0.000000]\n",
            "137: [discriminator loss: -1.459852, acc: 0.000000] [adversarial loss: -11.353433, acc: 0.000000]\n",
            "138: [discriminator loss: -1.500248, acc: 0.000000] [adversarial loss: -11.490808, acc: 0.000000]\n",
            "139: [discriminator loss: -1.521335, acc: 0.000000] [adversarial loss: -11.547104, acc: 0.000000]\n",
            "140: [discriminator loss: -1.545279, acc: 0.000000] [adversarial loss: -11.606801, acc: 0.000000]\n",
            "141: [discriminator loss: -1.534328, acc: 0.000000] [adversarial loss: -11.722956, acc: 0.000000]\n",
            "142: [discriminator loss: -1.561206, acc: 0.000000] [adversarial loss: -11.817638, acc: 0.000000]\n",
            "143: [discriminator loss: -1.587002, acc: 0.000000] [adversarial loss: -11.860817, acc: 0.000000]\n",
            "144: [discriminator loss: -1.594849, acc: 0.000000] [adversarial loss: -11.958722, acc: 0.000000]\n",
            "145: [discriminator loss: -1.609683, acc: 0.000000] [adversarial loss: -12.068442, acc: 0.000000]\n",
            "146: [discriminator loss: -1.615223, acc: 0.000000] [adversarial loss: -11.987409, acc: 0.000000]\n",
            "147: [discriminator loss: -1.680965, acc: 0.000000] [adversarial loss: -12.059160, acc: 0.000000]\n",
            "148: [discriminator loss: -1.677227, acc: 0.000000] [adversarial loss: -12.117756, acc: 0.000000]\n",
            "149: [discriminator loss: -1.681678, acc: 0.000000] [adversarial loss: -12.142334, acc: 0.000000]\n",
            "150: [discriminator loss: -1.710000, acc: 0.000000] [adversarial loss: -12.139626, acc: 0.000000]\n",
            "151: [discriminator loss: -1.726798, acc: 0.000000] [adversarial loss: -12.131659, acc: 0.000000]\n",
            "152: [discriminator loss: -1.737519, acc: 0.000000] [adversarial loss: -12.243912, acc: 0.000000]\n",
            "153: [discriminator loss: -1.812326, acc: 0.000000] [adversarial loss: -12.272882, acc: 0.000000]\n",
            "154: [discriminator loss: -1.804074, acc: 0.000000] [adversarial loss: -12.312118, acc: 0.000000]\n",
            "155: [discriminator loss: -1.820756, acc: 0.000000] [adversarial loss: -12.385735, acc: 0.000000]\n",
            "156: [discriminator loss: -1.861557, acc: 0.000000] [adversarial loss: -12.296562, acc: 0.000000]\n",
            "157: [discriminator loss: -1.890279, acc: 0.000000] [adversarial loss: -12.339350, acc: 0.000000]\n",
            "158: [discriminator loss: -1.849867, acc: 0.000000] [adversarial loss: -12.299431, acc: 0.000000]\n",
            "159: [discriminator loss: -1.882763, acc: 0.000000] [adversarial loss: -12.444849, acc: 0.000000]\n",
            "160: [discriminator loss: -1.870274, acc: 0.000000] [adversarial loss: -12.368270, acc: 0.000000]\n",
            "161: [discriminator loss: -1.907390, acc: 0.000000] [adversarial loss: -12.372475, acc: 0.000000]\n",
            "162: [discriminator loss: -1.866344, acc: 0.000000] [adversarial loss: -12.360743, acc: 0.000000]\n",
            "163: [discriminator loss: -1.907202, acc: 0.000000] [adversarial loss: -12.509357, acc: 0.000000]\n",
            "164: [discriminator loss: -1.939013, acc: 0.000000] [adversarial loss: -12.528640, acc: 0.000000]\n",
            "165: [discriminator loss: -1.952124, acc: 0.000000] [adversarial loss: -12.504190, acc: 0.000000]\n",
            "166: [discriminator loss: -1.998333, acc: 0.000000] [adversarial loss: -12.492426, acc: 0.000000]\n",
            "167: [discriminator loss: -2.031387, acc: 0.000000] [adversarial loss: -12.474220, acc: 0.000000]\n",
            "168: [discriminator loss: -2.042047, acc: 0.000000] [adversarial loss: -12.570273, acc: 0.000000]\n",
            "169: [discriminator loss: -2.067870, acc: 0.000000] [adversarial loss: -12.588644, acc: 0.000000]\n",
            "170: [discriminator loss: -2.129075, acc: 0.000000] [adversarial loss: -12.494270, acc: 0.000000]\n",
            "171: [discriminator loss: -2.122782, acc: 0.000000] [adversarial loss: -12.480240, acc: 0.000000]\n",
            "172: [discriminator loss: -2.254188, acc: 0.000000] [adversarial loss: -12.400372, acc: 0.000000]\n",
            "173: [discriminator loss: -2.380195, acc: 0.000000] [adversarial loss: -12.495472, acc: 0.000000]\n",
            "174: [discriminator loss: -2.397096, acc: 0.000000] [adversarial loss: -12.368645, acc: 0.000000]\n",
            "175: [discriminator loss: -2.551099, acc: 0.000000] [adversarial loss: -12.350534, acc: 0.000000]\n",
            "176: [discriminator loss: -2.586216, acc: 0.000000] [adversarial loss: -12.273829, acc: 0.000000]\n",
            "177: [discriminator loss: -2.682869, acc: 0.000000] [adversarial loss: -12.223053, acc: 0.000000]\n",
            "178: [discriminator loss: -2.734760, acc: 0.000000] [adversarial loss: -12.060818, acc: 0.000000]\n",
            "179: [discriminator loss: -2.848522, acc: 0.000000] [adversarial loss: -11.959249, acc: 0.000000]\n",
            "180: [discriminator loss: -2.892370, acc: 0.000000] [adversarial loss: -11.766644, acc: 0.000000]\n",
            "181: [discriminator loss: -3.022023, acc: 0.000000] [adversarial loss: -11.670630, acc: 0.000000]\n",
            "182: [discriminator loss: -3.102360, acc: 0.000000] [adversarial loss: -11.666656, acc: 0.000000]\n",
            "183: [discriminator loss: -3.152428, acc: 0.000000] [adversarial loss: -11.419928, acc: 0.000000]\n",
            "184: [discriminator loss: -3.191253, acc: 0.000000] [adversarial loss: -11.300604, acc: 0.000000]\n",
            "185: [discriminator loss: -3.240768, acc: 0.000000] [adversarial loss: -11.213152, acc: 0.000000]\n",
            "186: [discriminator loss: -3.332152, acc: 0.000000] [adversarial loss: -11.093821, acc: 0.000000]\n",
            "187: [discriminator loss: -3.364589, acc: 0.000000] [adversarial loss: -10.976881, acc: 0.000000]\n",
            "188: [discriminator loss: -3.407528, acc: 0.000000] [adversarial loss: -11.067943, acc: 0.000000]\n",
            "189: [discriminator loss: -3.434339, acc: 0.000000] [adversarial loss: -11.098227, acc: 0.000000]\n",
            "190: [discriminator loss: -3.436188, acc: 0.000000] [adversarial loss: -11.143819, acc: 0.000000]\n",
            "191: [discriminator loss: -3.299904, acc: 0.000000] [adversarial loss: -10.972355, acc: 0.000000]\n",
            "192: [discriminator loss: -3.235290, acc: 0.000000] [adversarial loss: -11.098246, acc: 0.000000]\n",
            "193: [discriminator loss: -3.025710, acc: 0.000000] [adversarial loss: -11.196499, acc: 0.000000]\n",
            "194: [discriminator loss: -2.881962, acc: 0.000000] [adversarial loss: -11.339615, acc: 0.000000]\n",
            "195: [discriminator loss: -2.650317, acc: 0.000000] [adversarial loss: -11.057590, acc: 0.000000]\n",
            "196: [discriminator loss: -2.559984, acc: 0.000000] [adversarial loss: -10.355970, acc: 0.000000]\n",
            "197: [discriminator loss: -2.616622, acc: 0.000000] [adversarial loss: -9.555952, acc: 0.000000]\n",
            "198: [discriminator loss: -2.615992, acc: 0.000000] [adversarial loss: -9.058121, acc: 0.000000]\n",
            "199: [discriminator loss: -2.674681, acc: 0.000000] [adversarial loss: -8.763914, acc: 0.000000]\n",
            "200: [discriminator loss: -2.733771, acc: 0.000000] [adversarial loss: -8.454258, acc: 0.000000]\n",
            "201: [discriminator loss: -2.723749, acc: 0.000000] [adversarial loss: -8.450727, acc: 0.000000]\n",
            "202: [discriminator loss: -2.840843, acc: 0.000000] [adversarial loss: -8.371473, acc: 0.000000]\n",
            "203: [discriminator loss: -2.859069, acc: 0.000000] [adversarial loss: -8.606467, acc: 0.000000]\n",
            "204: [discriminator loss: -2.667181, acc: 0.000000] [adversarial loss: -8.602497, acc: 0.000000]\n",
            "205: [discriminator loss: -2.708456, acc: 0.000000] [adversarial loss: -9.029627, acc: 0.000000]\n",
            "206: [discriminator loss: -2.634466, acc: 0.000000] [adversarial loss: -9.674841, acc: 0.000000]\n",
            "207: [discriminator loss: -2.434892, acc: 0.000000] [adversarial loss: -10.446030, acc: 0.000000]\n",
            "208: [discriminator loss: -2.320876, acc: 0.000000] [adversarial loss: -10.847872, acc: 0.000000]\n",
            "209: [discriminator loss: -2.071325, acc: 0.000000] [adversarial loss: -11.368370, acc: 0.000000]\n",
            "210: [discriminator loss: -1.869028, acc: 0.000000] [adversarial loss: -11.593253, acc: 0.000000]\n",
            "211: [discriminator loss: -1.833637, acc: 0.000000] [adversarial loss: -11.603521, acc: 0.000000]\n",
            "212: [discriminator loss: -1.805422, acc: 0.000000] [adversarial loss: -11.548529, acc: 0.000000]\n",
            "213: [discriminator loss: -1.874042, acc: 0.000000] [adversarial loss: -11.325673, acc: 0.000000]\n",
            "214: [discriminator loss: -2.046118, acc: 0.000000] [adversarial loss: -10.865787, acc: 0.000000]\n",
            "215: [discriminator loss: -2.147086, acc: 0.000000] [adversarial loss: -10.618643, acc: 0.000000]\n",
            "216: [discriminator loss: -2.364735, acc: 0.000000] [adversarial loss: -10.597980, acc: 0.000000]\n",
            "217: [discriminator loss: -2.468387, acc: 0.000000] [adversarial loss: -10.589868, acc: 0.000000]\n",
            "218: [discriminator loss: -2.394267, acc: 0.000000] [adversarial loss: -10.891529, acc: 0.000000]\n",
            "219: [discriminator loss: -2.293432, acc: 0.000000] [adversarial loss: -11.618809, acc: 0.000000]\n",
            "220: [discriminator loss: -2.242419, acc: 0.000000] [adversarial loss: -12.268373, acc: 0.000000]\n",
            "221: [discriminator loss: -2.090231, acc: 0.000000] [adversarial loss: -13.008357, acc: 0.000000]\n",
            "222: [discriminator loss: -1.997153, acc: 0.000000] [adversarial loss: -13.615266, acc: 0.000000]\n",
            "223: [discriminator loss: -1.871570, acc: 0.000000] [adversarial loss: -14.033485, acc: 0.000000]\n",
            "224: [discriminator loss: -1.751549, acc: 0.000000] [adversarial loss: -14.282976, acc: 0.000000]\n",
            "225: [discriminator loss: -1.801398, acc: 0.000000] [adversarial loss: -14.377687, acc: 0.000000]\n",
            "226: [discriminator loss: -1.814034, acc: 0.000000] [adversarial loss: -14.197735, acc: 0.000000]\n",
            "227: [discriminator loss: -1.889128, acc: 0.000000] [adversarial loss: -13.664471, acc: 0.000000]\n",
            "228: [discriminator loss: -1.976320, acc: 0.000000] [adversarial loss: -13.823371, acc: 0.000000]\n",
            "229: [discriminator loss: -2.029325, acc: 0.000000] [adversarial loss: -13.451445, acc: 0.000000]\n",
            "230: [discriminator loss: -2.061110, acc: 0.000000] [adversarial loss: -13.039867, acc: 0.000000]\n",
            "231: [discriminator loss: -2.125810, acc: 0.000000] [adversarial loss: -13.048010, acc: 0.000000]\n",
            "232: [discriminator loss: -2.184919, acc: 0.000000] [adversarial loss: -12.784090, acc: 0.000000]\n",
            "233: [discriminator loss: -2.170277, acc: 0.000000] [adversarial loss: -12.953896, acc: 0.000000]\n",
            "234: [discriminator loss: -2.161684, acc: 0.000000] [adversarial loss: -12.786599, acc: 0.000000]\n",
            "235: [discriminator loss: -2.067928, acc: 0.000000] [adversarial loss: -12.751013, acc: 0.000000]\n",
            "236: [discriminator loss: -2.226532, acc: 0.000000] [adversarial loss: -12.823702, acc: 0.000000]\n",
            "237: [discriminator loss: -2.179268, acc: 0.000000] [adversarial loss: -12.396545, acc: 0.000000]\n",
            "238: [discriminator loss: -2.194735, acc: 0.000000] [adversarial loss: -12.269486, acc: 0.000000]\n",
            "239: [discriminator loss: -2.261711, acc: 0.000000] [adversarial loss: -12.438745, acc: 0.000000]\n",
            "240: [discriminator loss: -2.296410, acc: 0.000000] [adversarial loss: -12.360576, acc: 0.000000]\n",
            "241: [discriminator loss: -2.430294, acc: 0.000000] [adversarial loss: -12.593353, acc: 0.000000]\n",
            "242: [discriminator loss: -2.380647, acc: 0.000000] [adversarial loss: -12.413548, acc: 0.000000]\n",
            "243: [discriminator loss: -2.445655, acc: 0.000000] [adversarial loss: -12.488997, acc: 0.000000]\n",
            "244: [discriminator loss: -2.387422, acc: 0.000000] [adversarial loss: -12.550044, acc: 0.000000]\n",
            "245: [discriminator loss: -2.441634, acc: 0.000000] [adversarial loss: -12.486082, acc: 0.000000]\n",
            "246: [discriminator loss: -2.464225, acc: 0.000000] [adversarial loss: -12.304661, acc: 0.000000]\n",
            "247: [discriminator loss: -2.520178, acc: 0.000000] [adversarial loss: -12.367672, acc: 0.000000]\n",
            "248: [discriminator loss: -2.562704, acc: 0.000000] [adversarial loss: -12.457380, acc: 0.000000]\n",
            "249: [discriminator loss: -2.487133, acc: 0.000000] [adversarial loss: -12.615883, acc: 0.000000]\n",
            "250: [discriminator loss: -2.521630, acc: 0.000000] [adversarial loss: -12.738319, acc: 0.000000]\n",
            "251: [discriminator loss: -2.468301, acc: 0.000000] [adversarial loss: -12.632729, acc: 0.000000]\n",
            "252: [discriminator loss: -2.470901, acc: 0.000000] [adversarial loss: -12.689728, acc: 0.000000]\n",
            "253: [discriminator loss: -2.444896, acc: 0.000000] [adversarial loss: -12.762065, acc: 0.000000]\n",
            "254: [discriminator loss: -2.425047, acc: 0.000000] [adversarial loss: -12.958982, acc: 0.000000]\n",
            "255: [discriminator loss: -2.380060, acc: 0.000000] [adversarial loss: -13.058994, acc: 0.000000]\n",
            "256: [discriminator loss: -2.441764, acc: 0.000000] [adversarial loss: -12.826244, acc: 0.000000]\n",
            "257: [discriminator loss: -2.439800, acc: 0.000000] [adversarial loss: -12.989495, acc: 0.000000]\n",
            "258: [discriminator loss: -2.417737, acc: 0.000000] [adversarial loss: -12.992168, acc: 0.000000]\n",
            "259: [discriminator loss: -2.407968, acc: 0.000000] [adversarial loss: -13.179648, acc: 0.000000]\n",
            "260: [discriminator loss: -2.387405, acc: 0.000000] [adversarial loss: -12.971897, acc: 0.000000]\n",
            "261: [discriminator loss: -2.499243, acc: 0.000000] [adversarial loss: -12.962387, acc: 0.000000]\n",
            "262: [discriminator loss: -2.583167, acc: 0.000000] [adversarial loss: -12.921390, acc: 0.000000]\n",
            "263: [discriminator loss: -2.604886, acc: 0.000000] [adversarial loss: -12.630869, acc: 0.000000]\n",
            "264: [discriminator loss: -2.569078, acc: 0.000000] [adversarial loss: -12.383920, acc: 0.000000]\n",
            "265: [discriminator loss: -2.585867, acc: 0.000000] [adversarial loss: -12.210707, acc: 0.000000]\n",
            "266: [discriminator loss: -2.618095, acc: 0.000000] [adversarial loss: -11.839204, acc: 0.000000]\n",
            "267: [discriminator loss: -2.748913, acc: 0.000000] [adversarial loss: -11.727076, acc: 0.000000]\n",
            "268: [discriminator loss: -2.749520, acc: 0.000000] [adversarial loss: -11.166464, acc: 0.000000]\n",
            "269: [discriminator loss: -2.739267, acc: 0.000000] [adversarial loss: -11.302347, acc: 0.000000]\n",
            "270: [discriminator loss: -2.890592, acc: 0.000000] [adversarial loss: -11.179810, acc: 0.000000]\n",
            "271: [discriminator loss: -2.851031, acc: 0.000000] [adversarial loss: -10.953817, acc: 0.000000]\n",
            "272: [discriminator loss: -2.780817, acc: 0.000000] [adversarial loss: -10.969687, acc: 0.000000]\n",
            "273: [discriminator loss: -2.851268, acc: 0.000000] [adversarial loss: -11.004760, acc: 0.000000]\n",
            "274: [discriminator loss: -2.975373, acc: 0.000000] [adversarial loss: -10.676464, acc: 0.000000]\n",
            "275: [discriminator loss: -2.899852, acc: 0.000000] [adversarial loss: -10.645559, acc: 0.000000]\n",
            "276: [discriminator loss: -2.897749, acc: 0.000000] [adversarial loss: -10.491810, acc: 0.000000]\n",
            "277: [discriminator loss: -2.946174, acc: 0.000000] [adversarial loss: -10.377967, acc: 0.000000]\n",
            "278: [discriminator loss: -3.037073, acc: 0.000000] [adversarial loss: -10.161761, acc: 0.000000]\n",
            "279: [discriminator loss: -3.011408, acc: 0.000000] [adversarial loss: -10.224974, acc: 0.000000]\n",
            "280: [discriminator loss: -2.945577, acc: 0.000000] [adversarial loss: -10.307307, acc: 0.000000]\n",
            "281: [discriminator loss: -3.054286, acc: 0.000000] [adversarial loss: -10.510880, acc: 0.000000]\n",
            "282: [discriminator loss: -3.133926, acc: 0.000000] [adversarial loss: -10.045408, acc: 0.000000]\n",
            "283: [discriminator loss: -3.021776, acc: 0.000000] [adversarial loss: -10.079317, acc: 0.000000]\n",
            "284: [discriminator loss: -3.086076, acc: 0.000000] [adversarial loss: -10.298923, acc: 0.000000]\n",
            "285: [discriminator loss: -3.145619, acc: 0.000000] [adversarial loss: -10.295718, acc: 0.000000]\n",
            "286: [discriminator loss: -3.161899, acc: 0.000000] [adversarial loss: -9.992524, acc: 0.000000]\n",
            "287: [discriminator loss: -3.025269, acc: 0.000000] [adversarial loss: -10.206474, acc: 0.000000]\n",
            "288: [discriminator loss: -3.111980, acc: 0.000000] [adversarial loss: -10.012117, acc: 0.000000]\n",
            "289: [discriminator loss: -3.062822, acc: 0.000000] [adversarial loss: -10.128779, acc: 0.000000]\n",
            "290: [discriminator loss: -3.034857, acc: 0.000000] [adversarial loss: -10.081172, acc: 0.000000]\n",
            "291: [discriminator loss: -3.069634, acc: 0.000000] [adversarial loss: -9.849068, acc: 0.000000]\n",
            "292: [discriminator loss: -3.127844, acc: 0.000000] [adversarial loss: -9.920500, acc: 0.000000]\n",
            "293: [discriminator loss: -3.059868, acc: 0.000000] [adversarial loss: -9.977656, acc: 0.000000]\n",
            "294: [discriminator loss: -3.053888, acc: 0.000000] [adversarial loss: -9.682306, acc: 0.000000]\n",
            "295: [discriminator loss: -2.979283, acc: 0.000000] [adversarial loss: -9.911912, acc: 0.000000]\n",
            "296: [discriminator loss: -3.101802, acc: 0.000000] [adversarial loss: -9.820734, acc: 0.000000]\n",
            "297: [discriminator loss: -3.130014, acc: 0.000000] [adversarial loss: -9.903181, acc: 0.000000]\n",
            "298: [discriminator loss: -2.991991, acc: 0.000000] [adversarial loss: -10.174055, acc: 0.000000]\n",
            "299: [discriminator loss: -2.995357, acc: 0.000000] [adversarial loss: -10.187757, acc: 0.000000]\n",
            "300: [discriminator loss: -2.994348, acc: 0.000000] [adversarial loss: -10.027682, acc: 0.000000]\n",
            "301: [discriminator loss: -3.020037, acc: 0.000000] [adversarial loss: -10.150373, acc: 0.000000]\n",
            "302: [discriminator loss: -3.116026, acc: 0.000000] [adversarial loss: -10.238133, acc: 0.000000]\n",
            "303: [discriminator loss: -2.980957, acc: 0.000000] [adversarial loss: -10.511620, acc: 0.000000]\n",
            "304: [discriminator loss: -2.921380, acc: 0.000000] [adversarial loss: -10.537683, acc: 0.000000]\n",
            "305: [discriminator loss: -2.980703, acc: 0.000000] [adversarial loss: -10.470411, acc: 0.000000]\n",
            "306: [discriminator loss: -2.992083, acc: 0.000000] [adversarial loss: -10.658064, acc: 0.000000]\n",
            "307: [discriminator loss: -2.653163, acc: 0.000000] [adversarial loss: -10.601814, acc: 0.000000]\n",
            "308: [discriminator loss: -2.831723, acc: 0.000000] [adversarial loss: -10.537003, acc: 0.000000]\n",
            "309: [discriminator loss: -2.879688, acc: 0.000000] [adversarial loss: -10.791688, acc: 0.000000]\n",
            "310: [discriminator loss: -2.999477, acc: 0.000000] [adversarial loss: -11.043047, acc: 0.000000]\n",
            "311: [discriminator loss: -2.925851, acc: 0.000000] [adversarial loss: -11.213833, acc: 0.000000]\n",
            "312: [discriminator loss: -2.694915, acc: 0.000000] [adversarial loss: -11.045620, acc: 0.000000]\n",
            "313: [discriminator loss: -2.952111, acc: 0.000000] [adversarial loss: -11.595394, acc: 0.000000]\n",
            "314: [discriminator loss: -2.806819, acc: 0.000000] [adversarial loss: -11.458942, acc: 0.000000]\n",
            "315: [discriminator loss: -2.632603, acc: 0.000000] [adversarial loss: -11.224887, acc: 0.000000]\n",
            "316: [discriminator loss: -2.703614, acc: 0.000000] [adversarial loss: -12.026859, acc: 0.000000]\n",
            "317: [discriminator loss: -2.634090, acc: 0.000000] [adversarial loss: -12.283745, acc: 0.000000]\n",
            "318: [discriminator loss: -2.666835, acc: 0.000000] [adversarial loss: -12.499041, acc: 0.000000]\n",
            "319: [discriminator loss: -2.602497, acc: 0.000000] [adversarial loss: -13.084294, acc: 0.000000]\n",
            "320: [discriminator loss: -2.516292, acc: 0.000000] [adversarial loss: -12.765012, acc: 0.000000]\n",
            "321: [discriminator loss: -2.146374, acc: 0.000000] [adversarial loss: -12.472982, acc: 0.000000]\n",
            "322: [discriminator loss: -2.021821, acc: 0.000000] [adversarial loss: -12.233326, acc: 0.000000]\n",
            "323: [discriminator loss: -1.999520, acc: 0.000000] [adversarial loss: -12.730897, acc: 0.000000]\n",
            "324: [discriminator loss: -2.157468, acc: 0.000000] [adversarial loss: -13.682758, acc: 0.000000]\n",
            "325: [discriminator loss: -1.756389, acc: 0.000000] [adversarial loss: -13.864252, acc: 0.000000]\n",
            "326: [discriminator loss: -1.748210, acc: 0.000000] [adversarial loss: -14.279531, acc: 0.000000]\n",
            "327: [discriminator loss: -1.526981, acc: 0.000000] [adversarial loss: -14.150681, acc: 0.000000]\n",
            "328: [discriminator loss: -1.610063, acc: 0.000000] [adversarial loss: -14.099421, acc: 0.000000]\n",
            "329: [discriminator loss: -1.300789, acc: 0.000000] [adversarial loss: -14.070560, acc: 0.000000]\n",
            "330: [discriminator loss: -1.091479, acc: 0.000000] [adversarial loss: -13.335089, acc: 0.000000]\n",
            "331: [discriminator loss: -1.363745, acc: 0.000000] [adversarial loss: -15.046036, acc: 0.000000]\n",
            "332: [discriminator loss: -0.918465, acc: 0.000000] [adversarial loss: -14.726028, acc: 0.000000]\n",
            "333: [discriminator loss: -0.866079, acc: 0.000000] [adversarial loss: -15.018690, acc: 0.000000]\n",
            "334: [discriminator loss: -1.076206, acc: 0.000000] [adversarial loss: -15.690042, acc: 0.000000]\n",
            "335: [discriminator loss: -0.747597, acc: 0.000000] [adversarial loss: -15.648808, acc: 0.000000]\n",
            "336: [discriminator loss: -0.477088, acc: 0.000000] [adversarial loss: -15.929791, acc: 0.000000]\n",
            "337: [discriminator loss: -0.549014, acc: 0.000000] [adversarial loss: -16.631338, acc: 0.000000]\n",
            "338: [discriminator loss: -0.466948, acc: 0.000000] [adversarial loss: -17.031288, acc: 0.000000]\n",
            "339: [discriminator loss: -0.349546, acc: 0.000000] [adversarial loss: -16.784416, acc: 0.000000]\n",
            "340: [discriminator loss: -0.141720, acc: 0.000000] [adversarial loss: -16.496086, acc: 0.000000]\n",
            "341: [discriminator loss: -0.446932, acc: 0.000000] [adversarial loss: -17.581455, acc: 0.000000]\n",
            "342: [discriminator loss: -0.149479, acc: 0.000000] [adversarial loss: -17.314371, acc: 0.000000]\n",
            "343: [discriminator loss: 0.003440, acc: 0.000000] [adversarial loss: -17.128880, acc: 0.000000]\n",
            "344: [discriminator loss: 0.207011, acc: 0.000000] [adversarial loss: -17.875072, acc: 0.000000]\n",
            "345: [discriminator loss: 0.094539, acc: 0.000000] [adversarial loss: -18.370035, acc: 0.000000]\n",
            "346: [discriminator loss: 0.063105, acc: 0.000000] [adversarial loss: -18.419762, acc: 0.000000]\n",
            "347: [discriminator loss: 0.249214, acc: 0.000000] [adversarial loss: -18.435383, acc: 0.000000]\n",
            "348: [discriminator loss: 0.449630, acc: 0.000000] [adversarial loss: -17.110123, acc: 0.000000]\n",
            "349: [discriminator loss: 0.467324, acc: 0.000000] [adversarial loss: -15.945834, acc: 0.000000]\n",
            "350: [discriminator loss: 0.649768, acc: 0.000000] [adversarial loss: -14.199073, acc: 0.000000]\n",
            "351: [discriminator loss: 0.308246, acc: 0.000000] [adversarial loss: -14.647209, acc: 0.000000]\n",
            "352: [discriminator loss: 0.056777, acc: 0.000000] [adversarial loss: -13.890997, acc: 0.000000]\n",
            "353: [discriminator loss: 0.103571, acc: 0.000000] [adversarial loss: -13.899457, acc: 0.000000]\n",
            "354: [discriminator loss: -0.098254, acc: 0.000000] [adversarial loss: -14.393044, acc: 0.000000]\n",
            "355: [discriminator loss: 0.012760, acc: 0.000000] [adversarial loss: -13.922224, acc: 0.000000]\n",
            "356: [discriminator loss: 0.005098, acc: 0.000000] [adversarial loss: -13.014034, acc: 0.000000]\n",
            "357: [discriminator loss: 0.023500, acc: 0.000000] [adversarial loss: -12.394192, acc: 0.000000]\n",
            "358: [discriminator loss: -0.096157, acc: 0.000000] [adversarial loss: -11.702158, acc: 0.000000]\n",
            "359: [discriminator loss: -0.227508, acc: 0.000000] [adversarial loss: -11.282597, acc: 0.000000]\n",
            "360: [discriminator loss: -0.389554, acc: 0.000000] [adversarial loss: -11.145150, acc: 0.000000]\n",
            "361: [discriminator loss: -0.435738, acc: 0.000000] [adversarial loss: -10.382747, acc: 0.000000]\n",
            "362: [discriminator loss: -0.449507, acc: 0.000000] [adversarial loss: -10.442940, acc: 0.000000]\n",
            "363: [discriminator loss: -0.573929, acc: 0.000000] [adversarial loss: -11.039595, acc: 0.000000]\n",
            "364: [discriminator loss: -0.531146, acc: 0.000000] [adversarial loss: -10.568160, acc: 0.000000]\n",
            "365: [discriminator loss: -0.741647, acc: 0.000000] [adversarial loss: -9.809131, acc: 0.000000]\n",
            "366: [discriminator loss: -0.827867, acc: 0.000000] [adversarial loss: -10.719725, acc: 0.000000]\n",
            "367: [discriminator loss: -0.708156, acc: 0.000000] [adversarial loss: -10.695721, acc: 0.000000]\n",
            "368: [discriminator loss: -0.738986, acc: 0.000000] [adversarial loss: -10.785147, acc: 0.000000]\n",
            "369: [discriminator loss: -0.497310, acc: 0.000000] [adversarial loss: -10.788363, acc: 0.000000]\n",
            "370: [discriminator loss: -0.828166, acc: 0.000000] [adversarial loss: -10.256630, acc: 0.000000]\n",
            "371: [discriminator loss: -0.802864, acc: 0.000000] [adversarial loss: -10.195723, acc: 0.000000]\n",
            "372: [discriminator loss: -0.823804, acc: 0.000000] [adversarial loss: -10.500208, acc: 0.000000]\n",
            "373: [discriminator loss: -0.877520, acc: 0.000000] [adversarial loss: -11.350391, acc: 0.000000]\n",
            "374: [discriminator loss: -0.869933, acc: 0.000000] [adversarial loss: -11.110157, acc: 0.000000]\n",
            "375: [discriminator loss: -0.643183, acc: 0.000000] [adversarial loss: -10.573415, acc: 0.000000]\n",
            "376: [discriminator loss: -0.873147, acc: 0.000000] [adversarial loss: -10.914909, acc: 0.000000]\n",
            "377: [discriminator loss: -0.953329, acc: 0.000000] [adversarial loss: -11.111773, acc: 0.000000]\n",
            "378: [discriminator loss: -0.921131, acc: 0.000000] [adversarial loss: -11.112593, acc: 0.000000]\n",
            "379: [discriminator loss: -0.757163, acc: 0.000000] [adversarial loss: -10.512987, acc: 0.000000]\n",
            "380: [discriminator loss: -0.952933, acc: 0.000000] [adversarial loss: -10.615931, acc: 0.000000]\n",
            "381: [discriminator loss: -0.804244, acc: 0.000000] [adversarial loss: -9.731970, acc: 0.000000]\n",
            "382: [discriminator loss: -1.003874, acc: 0.000000] [adversarial loss: -9.341930, acc: 0.000000]\n",
            "383: [discriminator loss: -1.102617, acc: 0.000000] [adversarial loss: -9.454104, acc: 0.000000]\n",
            "384: [discriminator loss: -1.130666, acc: 0.000000] [adversarial loss: -10.249275, acc: 0.000000]\n",
            "385: [discriminator loss: -1.058827, acc: 0.000000] [adversarial loss: -10.292898, acc: 0.000000]\n",
            "386: [discriminator loss: -1.082987, acc: 0.000000] [adversarial loss: -9.354966, acc: 0.000000]\n",
            "387: [discriminator loss: -1.073763, acc: 0.000000] [adversarial loss: -9.136560, acc: 0.000000]\n",
            "388: [discriminator loss: -1.124468, acc: 0.000000] [adversarial loss: -9.806173, acc: 0.000000]\n",
            "389: [discriminator loss: -1.089730, acc: 0.000000] [adversarial loss: -9.185948, acc: 0.000000]\n",
            "390: [discriminator loss: -1.194926, acc: 0.000000] [adversarial loss: -9.898972, acc: 0.000000]\n",
            "391: [discriminator loss: -1.014385, acc: 0.000000] [adversarial loss: -9.772835, acc: 0.000000]\n",
            "392: [discriminator loss: -1.148885, acc: 0.000000] [adversarial loss: -9.855038, acc: 0.000000]\n",
            "393: [discriminator loss: -0.898248, acc: 0.000000] [adversarial loss: -9.528879, acc: 0.000000]\n",
            "394: [discriminator loss: -1.113277, acc: 0.000000] [adversarial loss: -10.297852, acc: 0.000000]\n",
            "395: [discriminator loss: -0.953910, acc: 0.000000] [adversarial loss: -10.080729, acc: 0.000000]\n",
            "396: [discriminator loss: -0.857491, acc: 0.000000] [adversarial loss: -9.847601, acc: 0.000000]\n",
            "397: [discriminator loss: -0.817456, acc: 0.000000] [adversarial loss: -9.958172, acc: 0.000000]\n",
            "398: [discriminator loss: -0.646874, acc: 0.000000] [adversarial loss: -9.911844, acc: 0.000000]\n",
            "399: [discriminator loss: -0.614171, acc: 0.000000] [adversarial loss: -10.483439, acc: 0.000000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-f1HiOC1tya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}