{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_WGAN .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFfCT+GoH2eb5fu3NUrs0y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syeong1218/keras-fig/blob/master/keras_WGAN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGeCxQfuNv4W",
        "colab_type": "text"
      },
      "source": [
        "# 5장 개선된 GAN 모델\n",
        "\n",
        "- WGAN의 이론적 방정식\n",
        "- LSGAN 이론\n",
        "- ACGAN 이론\n",
        "- 케라스를 사용해 개선된 GAN 모델을 구현하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyaLerHLN0WG",
        "colab_type": "text"
      },
      "source": [
        "### GAN\n",
        "판별기는 진짜 데이터에서 가짜 데이터를 정확하게 분류해내려고 하고, 생성기는 판별기를 속이려한다. 판별기와 생성기가 서로 상반된 목표를 가지고 있어 훈련할 때 쉽게 불안정해진다.\n",
        "\n",
        "GAN의 목표는 생성기 데이터 분포를 진짜 데이터 분포로 만드는 것이다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPEAUtDTXFG",
        "colab_type": "text"
      },
      "source": [
        "###  WGAN\n",
        "\n",
        "판별기와 생성기를 교대로 훈련시킨다. 생성기를 1회 훈련시키기 전에 판별기를 $n_{critic}$회 훈련시킨다. 이는 생성기와 판별기를 동일한 횟수로 훈련시키는 GAN과 다른 점이다.\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/keras-fig/blob/master/5.1.3.JPG?raw=true)\n",
        "> **알고리즘**<br>\n",
        "  매개변수 : $\\alpha=0.00005, c=0.01, m=64, n_{critic}=5$ <br>\n",
        "  조건 : $\\alpha$- 학습속도, c-클리핑 매개변수, m-배치크기, $n_{critic}$-생성기가 1회 반복하는 동안 판별기의 반복 횟수<br>\n",
        "  조건 : $w_0$-초기 판별기 매개변수, $\\theta_0$-초기 생성기 매개변수<br>\n",
        "  1. while $\\theta$가 수렴하지 않는 동안 do\n",
        "  2. for t=1,...,$n_{critic}$ do\n",
        "  3. 실제 데이터에서 배치 ${(x^{(i)})}_{i=1}^m \\sim p_{data}$를 샘플링\n",
        "  4. 균등 노이즈 분포에서 배치 ${(z^{(i)})}_{i=1}^m \\sim p(z)$를 샘플링\n",
        "  5. 판별기 경사를 계산,$g_w\\leftarrow \\triangledown _w\\left [ -\\frac{1}{m}\\sum_{i=1}^{m}D_w(x^{(i)})+\\frac{1}{m}\\sum_{i=1}^{m}D_w(g_\\theta(z^{(i)})) \\right ]$\n",
        "  6. 판별기 매개변수 업데이트,$w\\leftarrow w-\\alpha \\times RMSProp(w,g_w)$\n",
        "  7. 판별기 가중치 제한, $w\\leftarrow clip(w,-c,c)$\n",
        "  8. end for\n",
        "  9. 균등 노이즈 분포에서 배치 ${(z^{(i)})}_{i=1}^m \\sim p(z)$를 샘플링\n",
        "  10. 생성기 경사 계산, $g_{\\theta}\\leftarrow \\triangledown _{\\theta}\\frac{1}{m}\\sum_{i=1}^{m}D_w(g_\\theta(z^{(i)}))$\n",
        "  11. 생성기 매개변수 업데이트, $\\theta\\leftarrow \\theta-\\alpha \\times RMSProp(w,g_{\\theta})$\n",
        "  12. end while\n",
        ".\n",
        "\n",
        "판별기를 훈련시킨다는 것은 판별기의 매개변수를 학습한다는 뜻이다.\n",
        ">>**판별기 훈련**\n",
        "<br>3. 실제 데이터에서 배치를 샘플링\n",
        " <br>4. 가짜 데이터에서 배치를 샘플링\n",
        " <br>5. 샘플링된 데이터를 판별기 네트워크에 공급한 다음, 판별기 매개변수의 경사를 계산\n",
        " <br>6. 판별기 매개변수는 RMSProp를 사용해 최적화\n",
        " <br>7. EMD 최적화에서 판별기 매개변수를 특정 범위 내로 제한해서 립시츠의 제약 조건을 적용\n",
        " <br><br>실제 데이터를 사용해 훈련할 때 손실 함수를 최소화하기 위해 $y_{prediction}=D_{w}(x)$를 증가시킨다. 가짜 데이터를 사용해 훈련시키는 경우, $y_{prediction}=D_{w}(G(x))$를 감소시켜 손실 함수를 최소화한다.\n",
        "\n",
        ">>**생성기 훈련**\n",
        " <br>9. 가짜 데이터 배치를 샘플링\n",
        " <br>10. 생성기 경사 계산\n",
        " <br>11. RMSProp를 사용해 최적화\n",
        " <br><br> $y_{prediction}=D_{w}(G(x))$를 증가시켜 훈련하는 동안 가짜 데이터에 진짜 레이블이 달려 있을 때 손실함수를 최소화한다. \n",
        " \n",
        "생성기 훈련이 끝나면 판별기 매개변수를 고정했던 것을 풀고 다시 판별기 훈련을 $n_{critic}$회 시작한다. \n",
        "\n",
        "생성기는 데이터 위조에만 관여하기 때문에 판별기를 훈련하는 동안 생성기 매개변수를 고정할 필요가 없다.\n",
        "\n",
        "판별기 경사를 계산할 때 실제 데이터의 레이블은 1.0이지만 가짜 데이터의 레이블은 -1.0이 된다.\n",
        "\n",
        "\n",
        "- RMSProp 사용 이유\n",
        "\n",
        "  critic을 학습 할 때 Adam과 같은 mometum 베이스 optimizer를 사용하면 학습이 불안정하다 \n",
        "\n",
        "  그 이유는 loss값이 튀고 샘플이 좋지 않은 경우(일반적으로 학습 초반) Adam이 가고자 하는 방향, 즉 이전에 기억했던 방향(Adam step)과 gradient의 방향 간의 cosine값이 음수가 된다. 일반적으로 nonstationary 문제(극한값이 존재하지 않음)에 대해서는 momentum계열보다 RMSProp이 성능이 더 좋다고 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0NabfsYNuEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from lib import gan\n",
        "\n",
        "def train(models, x_train, params):\n",
        "    \"\"\"판별기와 적대적 네트워크 훈련\n",
        "    판별기와 적대적 네트워크를 배치 단위로 교대로 훈련\n",
        "    먼저 판별기가 제대로 레이블이 붙은 진짜와 가짜 이미지를 사용해 n_critic회 훈련됨\n",
        "    판별기 가중치는 립시츠 제약 조건에 따라 범위가 제한됨\n",
        "    다음으로 생성기가 진짜인 척하는 가짜 이미지를 사용해(적대적 네트워크를 통해) 훈련됨\n",
        "    save_interval마다 샘플 이미지를 생성\n",
        "\n",
        "    #인수\n",
        "     models(list) : Generator, discriminator, Adversarial 모델\n",
        "     x_train(tensor) : 이미지 훈련\n",
        "     params(list) : 네트워크 매개변수\n",
        "    \"\"\"\n",
        "    # GAN 모델\n",
        "    generator, discriminator, adversarial = models\n",
        "    # 네트워크 매개변수\n",
        "    (batch_size, latent_size, n_critic, \n",
        "            clip_value, train_steps, model_name) = params\n",
        "    # 생성기 이미지는 500 단계마다 저장됨\n",
        "    save_interval = 500\n",
        "    # 훈련하는 동안 생성기 출력이 어떻게 진화하는지 보기 위한 노이즈 벡터\n",
        "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
        "    # 훈련 데이터 세트의 요소 개수\n",
        "    train_size = x_train.shape[0]\n",
        "    # 실제 데이터의 레이블\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    for i in range(train_steps):\n",
        "        # 판별기를 n_critic회 훈련시킴\n",
        "        loss = 0\n",
        "        acc = 0\n",
        "        for _ in range(n_critic):\n",
        "            # 1 배치에 대해 판별기를 훈련\n",
        "            # 실제 이미지(label=1.0)와 가짜 이미지(label=-1.0)으로 구성된 1 배치\n",
        "            # 데이터세트에서 실제 이미지를 임의로 선정\n",
        "            rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
        "            real_images = x_train[rand_indexes]\n",
        "            # 생성기를 사용해 노이즈에서 가짜 이미지 생성\n",
        "            # 균등 분포를 사용해 노이즈 생성\n",
        "            noise = np.random.uniform(-1.0,\n",
        "                                      1.0,\n",
        "                                      size=[batch_size, latent_size])\n",
        "            fake_images = generator.predict(noise)\n",
        "\n",
        "            # 판별기 네트워크 훈련\n",
        "            # 진짜 데이터 레이블l=1, 가짜 데이터 레이블=-1\n",
        "            # 진짜와 가짜 이미지를 결합해 하나의 배치를 만드는 대신\n",
        "            # 처음에는 진짜 데이터로 구성된 하나의 배치로 훈련한 다음\n",
        "            # 가짜 이미지로 구성된 하나의 배치로 훈련\n",
        "            # 이렇게 바꿈으로써\n",
        "            # 진짜와 가짜 데이터 레이블의 부호가 반대고(+1과 -1)\n",
        "            # 점위 제한(클리핑)으로 인해 가중치의 크기가 작아서\n",
        "            # 경사가 소실되는 것을 방지\n",
        "            real_loss, real_acc = discriminator.train_on_batch(real_images,\n",
        "                                                               real_labels)\n",
        "            fake_loss, fake_acc = discriminator.train_on_batch(fake_images,\n",
        "                                                               -real_labels)\n",
        "            # 평균 손실과 정확도를 누적\n",
        "            loss += 0.5 * (real_loss + fake_loss)\n",
        "            acc += 0.5 * (real_acc + fake_acc)\n",
        "\n",
        "            # 립시츠 제약 사향을 만족하기 위해 판별치 가중치 범위 제한\n",
        "            for layer in discriminator.layers:\n",
        "                weights = layer.get_weights()\n",
        "                weights = [np.clip(weight,\n",
        "                                   -clip_value,\n",
        "                                   clip_value) for weight in weights]\n",
        "                layer.set_weights(weights)\n",
        "\n",
        "        # n_critic회 반복 훈련하는 동안 평균 손실과 정확도\n",
        "        loss /= n_critic\n",
        "        acc /= n_critic\n",
        "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
        "\n",
        "        # 1 배치 동안 적대적 네트워크 훈련\n",
        "        # label=1.0인 가짜 이미지의 1 배치\n",
        "        # 적대적 네트워크의 판별기 가중치가 고정되어 있으므로\n",
        "        # 생성기만 훈련됨\n",
        "        # 균등 분포를 사용해 노이즈 생성\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "        # 적대적 네트워크 훈련\n",
        "        # 판별기 훈련과 달리 변수에 가짜 이미지를 저장하지 않음\n",
        "        # 가짜 이미지는 분류를 위해 적대적 네트워크의 판별기 입력으로 전달됨\n",
        "        # 가짜 이미지는 진짜 레이블을 가지고 있음\n",
        "        # 손실과 정확도를 기록\n",
        "        loss, acc = adversarial.train_on_batch(noise, real_labels)\n",
        "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
        "        print(log)\n",
        "        if (i + 1) % save_interval == 0:\n",
        "            if (i + 1) == train_steps:\n",
        "                show = True\n",
        "            else:\n",
        "                show = False\n",
        "\n",
        "            # 주기적으로 생성기 이미지를 그림\n",
        "            gan.plot_images(generator,\n",
        "                            noise_input=noise_input,\n",
        "                            show=show,\n",
        "                            step=(i + 1),\n",
        "                            model_name=model_name)\n",
        "\n",
        "    # 생성기 훈련이 끝나면 모델을 저장\n",
        "    # 훈련기 생성기는 향후 MNIST 숫자 생성을 위해 재로딩될 수 있음\n",
        "    generator.save(model_name + \".h5\")\n",
        "\n",
        "\n",
        "def wasserstein_loss(y_label, y_pred):\n",
        "    return -K.mean(y_label * y_pred)\n",
        "\n",
        "\n",
        "def build_and_train_models():\n",
        "    # MNIST 데이터세트 로딩\n",
        "    (x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "    # CNN 데이터를 (28, 28, 1)로 형상을 변경하고 정규화함\n",
        "    image_size = x_train.shape[1]\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "    x_train = x_train.astype('float32') / 255\n",
        "\n",
        "    model_name = \"wgan_mnist\"\n",
        "    # 네트워크 매개변수\n",
        "    # 잠재 혹은 z벡터 차원은 100\n",
        "    latent_size = 100\n",
        "    # WGAN 논문[2]에서 참조한 매개변수\n",
        "    n_critic = 5\n",
        "    clip_value = 0.01\n",
        "    batch_size = 64\n",
        "    lr = 5e-5\n",
        "    train_steps = 40000\n",
        "    input_shape = (image_size, image_size, 1)\n",
        "\n",
        "    # 판별기 모델 구성\n",
        "    inputs = Input(shape=input_shape, name='discriminator_input')\n",
        "    # WGAN은 논문[2]의 선형 활성화를 사용\n",
        "    discriminator = gan.discriminator(inputs, activation='linear')\n",
        "    optimizer = RMSprop(lr=lr)\n",
        "    # WGAN 판별기는 베셔슈타인 손실을 사용\n",
        "    discriminator.compile(loss=wasserstein_loss,\n",
        "                          optimizer=optimizer,\n",
        "                          metrics=['accuracy'])\n",
        "    discriminator.summary()\n",
        "\n",
        "    # 생성기 모델 구성\n",
        "    input_shape = (latent_size, )\n",
        "    inputs = Input(shape=input_shape, name='z_input')\n",
        "    generator = gan.generator(inputs, image_size)\n",
        "    generator.summary()\n",
        "\n",
        "    # 적대적 모델 생성 = 생성기 + 판별기\n",
        "    # 적대적 네트워크를 훈련하는 동안 판별기의 가중치는 고정\n",
        "    discriminator.trainable = False\n",
        "    adversarial = Model(inputs,\n",
        "                        discriminator(generator(inputs)),\n",
        "                        name=model_name)\n",
        "    adversarial.compile(loss=wasserstein_loss,\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    adversarial.summary()\n",
        "\n",
        "    # 판별기와 적대적 네트워크를 훈련\n",
        "    models = (generator, discriminator, adversarial)\n",
        "    params = (batch_size,\n",
        "              latent_size,\n",
        "              n_critic,\n",
        "              clip_value,\n",
        "              train_steps,\n",
        "              model_name)\n",
        "    train(models, x_train, params)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    help_ = \"Load generator h5 model with trained weights\"\n",
        "    parser.add_argument(\"-g\", \"--generator\", help=help_)\n",
        "    args = parser.parse_args()\n",
        "    if args.generator:\n",
        "        generator = load_model(args.generator)\n",
        "        gan.test_generator(generator)\n",
        "    else:\n",
        "        build_and_train_models()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}